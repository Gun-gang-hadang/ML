{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=2.0 (from tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=2.0 from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/3d/5b/0ae95c2817e0c9a510581a84c1a029fd447c7d0f289cc8b753e6ad93c216/grpcio-1.56.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading grpcio-1.56.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting h5py>=2.9.0 (from tensorflow)\n",
      "  Downloading h5py-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.12,>=2.11.0 (from tensorflow)\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0 (from tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/ea/df/55525e489c43f9dbb6c8ea27d8a567b3dcd18a22f3c45483055f5ca6611d/libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/trdang/snap/jupyter/common/lib/python3.7/site-packages (from tensorflow) (1.21.6)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting packaging (from tensorflow)\n",
      "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<3.20,>=3.9.2 (from tensorflow)\n",
      "  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /snap/jupyter/6/lib/python3.7/site-packages (from tensorflow) (41.0.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /snap/jupyter/6/lib/python3.7/site-packages (from tensorflow) (1.12.0)\n",
      "Collecting tensorboard<2.12,>=2.11 (from tensorflow)\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0mm\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0 (from tensorflow)\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting typing-extensions>=3.6.6 (from tensorflow)\n",
      "  Obtaining dependency information for typing-extensions>=3.6.6 from https://files.pythonhosted.org/packages/ec/6b/63cc3df74987c36fe26157ee12e09e8f9db4de771e0f3404263117e75b95/typing_extensions-4.7.1-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.15.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/a4/c0/f9ac791c3f6f58a343b350894a3e92d44e53d20d7cf205988279ebcbc6e5/tensorflow_io_gcs_filesystem-0.33.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.33.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /snap/jupyter/6/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.33.4)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.12,>=2.11->tensorflow)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/9c/8d/bff87fc722553a5691d8514da5523c23547f3894189ba03b57592e37bdc2/google_auth-2.22.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.12,>=2.11->tensorflow)\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.12,>=2.11->tensorflow)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/1a/b5/228c1cdcfe138f1a8e01ab1b54284c8b83735476cb22b6ba251656ed13ad/Markdown-3.4.4-py3-none-any.whl.metadata\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting requests<3,>=2.21.0 (from tensorboard<2.12,>=2.11->tensorflow)\n",
      "  Obtaining dependency information for requests<3,>=2.21.0 from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.12,>=2.11->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.12,>=2.11->tensorflow)\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1 (from tensorboard<2.12,>=2.11->tensorflow)\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting urllib3<2.0 (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow)\n",
      "  Obtaining dependency information for urllib3<2.0 from https://files.pythonhosted.org/packages/c5/05/c214b32d21c0b465506f95c4f28ccbcba15022e000b043b72b3df7728471/urllib3-1.26.16-py2.py3-none-any.whl.metadata\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4 (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow)\n",
      "  Obtaining dependency information for importlib-metadata>=4.4 from https://files.pythonhosted.org/packages/ff/94/64287b38c7de4c90683630338cf28f129decbba0a44f0c6db35a873c73c4/importlib_metadata-6.7.0-py3-none-any.whl.metadata\n",
      "  Downloading importlib_metadata-6.7.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow)\n",
      "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/89/f5/88e9dd454756fea555198ddbe6fa40d6408ec4f10ad4f0a911e0b7e471e4/charset_normalizer-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading charset_normalizer-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow)\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /snap/jupyter/6/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2019.3.9)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow)\n",
      "  Obtaining dependency information for MarkupSafe>=2.1.1 from https://files.pythonhosted.org/packages/e5/dd/49576e803c0d974671e44fa78049217fcc68af3662a24f831525ed30e6c7/MarkupSafe-2.1.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading MarkupSafe-2.1.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow)\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow)\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading grpcio-1.56.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.33.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0mm\n",
      "\u001b[?25hUsing cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.8/181.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Downloading charset_normalizer-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (175 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.8/175.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
      "Downloading MarkupSafe-2.1.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, libclang, flatbuffers, zipp, wrapt, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, packaging, opt-einsum, oauthlib, MarkupSafe, keras, idna, h5py, grpcio, google-pasta, gast, charset-normalizer, cachetools, astunparse, absl-py, werkzeug, rsa, requests, pyasn1-modules, importlib-metadata, requests-oauthlib, markdown, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed MarkupSafe-2.1.3 absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 charset-normalizer-3.2.0 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.56.2 h5py-3.8.0 idna-3.4 importlib-metadata-6.7.0 keras-2.11.0 libclang-16.0.6 markdown-3.4.4 oauthlib-3.2.2 opt-einsum-3.3.0 packaging-23.1 protobuf-3.19.6 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-2.31.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.33.0 termcolor-2.3.0 typing-extensions-4.7.1 urllib3-1.26.16 werkzeug-2.2.3 wrapt-1.15.0 zipp-3.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pillow\n",
      "  Downloading Pillow-9.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pillow\n",
      "Successfully installed pillow-9.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/trdang/snap/jupyter/common/lib/python3.7/site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /home/trdang/snap/jupyter/common/lib/python3.7/site-packages (from scikit-learn) (1.21.6)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/trdang/snap/jupyter/common/lib/python3.7/site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/trdang/snap/jupyter/common/lib/python3.7/site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/trdang/snap/jupyter/common/lib/python3.7/site-packages (from scikit-learn) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/trdang/snap/jupyter/common/lib/python3.7/site-packages (23.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abce'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = 'abcde'\n",
    "test.replace('d','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import ImageFile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_folders(image_folder, json_folder, num_samples=1000):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "\n",
    "    for food_name in os.listdir(image_folder):\n",
    "        if food_name.startswith('.'):\n",
    "            continue\n",
    "        \n",
    "        food_image_folder = os.path.join(image_folder, food_name)\n",
    "        food_json_folder = os.path.join(json_folder, food_name)\n",
    "        \n",
    "        image_files = [file_name for file_name in os.listdir(food_image_folder) if file_name.endswith('.jpg')]\n",
    "\n",
    "        sampled_images = random.sample(image_files, min(num_samples, len(image_files)))\n",
    "\n",
    "        for file_name in sampled_images:\n",
    "            image_path = os.path.join(food_image_folder, file_name)\n",
    "            image_name = os.path.splitext(file_name)[0]\n",
    "\n",
    "            json_file_path = os.path.join(food_json_folder + ' json/', image_name + '.json')\n",
    "\n",
    "            with open(json_file_path, 'r') as f:\n",
    "                annotation = json.load(f)\n",
    "\n",
    "            label = annotation[0]['Name']\n",
    "            label = label.replace('-','')\n",
    "            if label is not None:\n",
    "                if label == 'bogsung-a':\n",
    "                    labels.append('peach')\n",
    "                elif label == 'gamjaseupeu':\n",
    "                    labels.append('potato_soup')\n",
    "                else:\n",
    "                    labels.append(label)\n",
    "            image_paths.append(image_path)\n",
    "\n",
    "    return image_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_folder = './Training/[원천]음식203_Tra/'\n",
    "# json_folder = './Training/[라벨]음식203_Tra_json/'\n",
    "image_folder = './fooddata/train/source/'\n",
    "json_folder = './fooddata/train/label/'\n",
    "\n",
    "image_paths, labels = load_dataset_from_folders(image_folder, json_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['apple', 'bab', 'baechugimchi', 'banana', 'bangultomato',\n",
       "       'beulokolli', 'biennasosiji', 'boiled_sweet_potatoes', 'chamoe',\n",
       "       'chikinneoges', 'dalggalbi', 'dalgyalhulai', 'dalgyaljjim',\n",
       "       'danhobag', 'doenjanggug', 'doenjangjjigae', 'eomuggug',\n",
       "       'galbijjim', 'gamjabokkeum', 'gochujangajji', 'gugsu',\n",
       "       'haemulbokkeum', 'hunjeyeoneo', 'jabchae', 'jomigim', 'jug',\n",
       "       'kongnamul', 'korean_style_pancake', 'mandugug', 'miyeoggug',\n",
       "       'miyeognamul', 'mugimchi', 'oi', 'oliloseugui', 'omeulles',\n",
       "       'papeulika', 'podo', 'ppang', 'roasted_fish', 'saendeuwichi',\n",
       "       'saengseongui', 'sagwa', 'salmeundalgyal', 'samgyeobsalgui',\n",
       "       'sangchu', 'scrambled_egg', 'seukeulaembeuldeuegeu',\n",
       "       'sundubujjigae', 'sunsaljjimdalg', 'tomato', 'tteoggug', 'uyu',\n",
       "       'vienna_sausage', 'yachaebokkeum', 'yangbaechussam',\n",
       "       'yangsangchusaelleodeu'], dtype='<U21')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_labels = np.unique(labels)\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_mapping = {\"gamjabokkeum\":\"감자볶음\",\n",
    "\"gochujangajji\":\"고추 장아찌\",\n",
    "\"chikinneoges\":\"치킨너겟\",\n",
    "\"galbijjim\":\"갈비찜\",\n",
    "\"mandugug\":\"만두국\",\n",
    "\"oliloseugui\":\"오리구이\",\n",
    "\"dalgyalhulai\":\"달걀후라이\",\n",
    "\"gugsu\":\"국수\",\n",
    "\"danhobag\":\"단호박찜\",\n",
    "\"dalgyalbokkeumbab\":\"볶음밥\",\n",
    "\"dalgyaljjim\":\"달걀찜\",\n",
    "\"dalgyalhulai\":\"달걀후라이\",\n",
    "\"dalggalbi\":\"닭갈비\",\n",
    "\"doenjanggug\":\"된장국\",\n",
    "\"doenjangjjigae\":\"된장찌개\",\n",
    "\"tteoggug\":\"떡국\",\n",
    "\"mugimchi\":\"무김치\",\n",
    "\"miyeoggug\":\"미역국\",\n",
    "\"miyeognamul\":\"미역나물\",\n",
    "\"banana\":\"바나나\",\n",
    "\"bab\":\"밥\",\n",
    "\"bangultomato\":\"방울토마토\",\n",
    "\"baechugimchi\":\"배추김치\",\n",
    "\"korean_style_pancake\":\"부침개\",\n",
    "\"beulokolli\":\"브로콜리\",\n",
    "\"biennasosiji\":\"비엔나 소시지\",\n",
    "\"vienna_sausage\":\"비엔나 소시지\",\n",
    "\"ppang\":\"빵\",\n",
    "\"sagwa\":\"사과\",\n",
    "\"apple\":\"사과\",\n",
    "\"boiled_sweet_potatoes\":\"고구마\",\n",
    "\"salmeundalgyal\":\"삶은 달걀\",\n",
    "\"samgyeobsalgui\":\"삼겹살구이\",\n",
    "\"sangchu\":\"상추\",\n",
    "\"saendeuwichi\":\"샌드위치\",\n",
    "\"roasted_fish\":\"생선구이\",\n",
    "\"saengseongui\":\"생선구이\",\n",
    "\"sundubujjigae\":\"순두부찌개\",\n",
    "\"sunsaljjimdalg\":\"찜닭\",\n",
    "\"scrambled_egg\":\"스크램블 에그\",\n",
    "\"seukeulaembeuldeuegeu\":\"스크램블 에그\",\n",
    "\"yachaebokkeum\":\"야채볶음\",\n",
    "\"yangbaechussam\":\"양배추쌈\",\n",
    "\"yangsangchusaelleodeu\":\"양상추샐러드\",\n",
    "\"eomuggug\":\"어묵국\",\n",
    "\"omeulles\":\"오믈렛\",\n",
    "\"oi\":\"오이\",\n",
    "\"uyu\":\"우유\",\n",
    "\"jabchae\":\"잡채\",\n",
    "\"jomigim\":\"조미김\",\n",
    "\"jug\":\"죽\",\n",
    "\"chamoe\":\"참외\",\n",
    "\"kongnamul\":\"콩나물 무침\",\n",
    "\"tomato\":\"토마토\",\n",
    "\"papeulika\":\"파프리카\",\n",
    "\"podo\":\"포도\",\n",
    "\"haemulbokkeum\":\"해물볶음\",\n",
    "\"hunjeyeoneo\":\"훈제연어\",\n",
    "\"hunjeoli\":\"훈제오리\"}\n",
    "len(label_mapping)\n",
    "# label_mapping = {'galbigu-i': '갈비구이', 'gamjasaelleodeu': '감자샐러드', 'ganjang': '간장', 'potato_soup': '감자스프',\n",
    "#                  'gamjagu-i': '감자구이','gamjabokk-eum':'감자볶음','gam':'감','tangerine_juice':'감귤주스','gacheudong':'가츠동',\n",
    "#                  'gaji':'가지구이', 'gamjageulatang':'감자그라탕'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_labels = [label_mapping[label] for label in labels]\n",
    "\n",
    "unique_korean_labels = np.unique(korean_labels)\n",
    "label_to_int = {label: i for i, label in enumerate(unique_korean_labels)}\n",
    "int_labels = np.array([label_to_int[label] for label in korean_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_paths, val_image_paths, train_labels, val_labels = train_test_split(image_paths, int_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    # Legacy Python that doesn't verify HTTPS certificates by default\n",
    "    pass\n",
    "else:\n",
    "    # Handle target environment that doesn't support HTTPS verification\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(len(unique_korean_labels), activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range = 20,\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    preprocessing_function = preprocess_input\n",
    ")\n",
    "\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab용\n",
    "# train_dir = '/content/drive/MyDrive/final/train'\n",
    "# val_dir = '/content/drive/MyDrive/final/val'\n",
    "# jupyter용\n",
    "train_dir = './food/final2/train'\n",
    "val_dir = './food/final2/val'\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Copy train images to the train directory\n",
    "for image_path in train_image_paths:\n",
    "    directory_path = os.path.dirname(image_path)\n",
    "    class_name = os.path.basename(directory_path)\n",
    "    destination_dir = os.path.join(train_dir, class_name)\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "    shutil.copy(image_path, destination_dir)\n",
    "\n",
    "# Copy validation images to the validation directory\n",
    "for image_path in val_image_paths:\n",
    "    directory_path = os.path.dirname(image_path)\n",
    "    class_name = os.path.basename(directory_path)\n",
    "    destination_dir = os.path.join(val_dir, class_name)\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "    shutil.copy(image_path, destination_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31410 images belonging to 52 classes.\n",
      "Found 7903 images belonging to 52 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size = (299, 299),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'sparse',\n",
    "    classes = unique_korean_labels.tolist(),\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size = (299, 299),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'sparse',\n",
    "    classes = unique_korean_labels.tolist()\n",
    ")\n",
    "\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "982/982 [==============================] - 75327s 77s/step - loss: 0.8961 - accuracy: 0.7352 - val_loss: 0.5731 - val_accuracy: 0.8242\n",
      "Epoch 2/10\n",
      "982/982 [==============================] - 72278s 74s/step - loss: 0.5487 - accuracy: 0.8277 - val_loss: 0.5249 - val_accuracy: 0.8358\n",
      "Epoch 3/10\n",
      "982/982 [==============================] - 86167s 88s/step - loss: 0.4544 - accuracy: 0.8544 - val_loss: 0.4429 - val_accuracy: 0.8617\n",
      "Epoch 4/10\n",
      "982/982 [==============================] - 79695s 81s/step - loss: 0.4007 - accuracy: 0.8705 - val_loss: 0.4368 - val_accuracy: 0.8669\n",
      "Epoch 5/10\n",
      "982/982 [==============================] - 85547s 87s/step - loss: 0.3534 - accuracy: 0.8842 - val_loss: 0.4068 - val_accuracy: 0.8703\n",
      "Epoch 6/10\n",
      "982/982 [==============================] - 84805s 86s/step - loss: 0.3193 - accuracy: 0.8957 - val_loss: 0.3819 - val_accuracy: 0.8824\n",
      "Epoch 7/10\n",
      "982/982 [==============================] - 84046s 86s/step - loss: 0.2963 - accuracy: 0.9037 - val_loss: 0.3358 - val_accuracy: 0.8989\n",
      "Epoch 8/10\n",
      "982/982 [==============================] - 86381s 88s/step - loss: 0.2700 - accuracy: 0.9116 - val_loss: 0.4039 - val_accuracy: 0.8770\n",
      "Epoch 9/10\n",
      "982/982 [==============================] - 84247s 86s/step - loss: 0.2550 - accuracy: 0.9150 - val_loss: 0.3003 - val_accuracy: 0.9093\n",
      "Epoch 10/10\n",
      "982/982 [==============================] - 83568s 85s/step - loss: 0.2374 - accuracy: 0.9199 - val_loss: 0.3318 - val_accuracy: 0.9019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f36901caac8>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "model.fit(train_generator, validation_data=val_generator, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247/247 [==============================] - 17006s 69s/step - loss: 0.3318 - accuracy: 0.9019\n",
      "Validation loss: 0.3318, Validation accuracy: 0.9019\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(val_generator)\n",
    "print(f\"Validation loss: {loss:.4f}, Validation accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./food_recognition_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('./food_recognition_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Make Predictions on New Images\n",
    "def predict_food_category(image_path):\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(299, 299))\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "\n",
    "    keys_with_value = []\n",
    "\n",
    "    prediction = model.predict(img)[0]\n",
    "    predicted_class_index = np.argmax(prediction)\n",
    "    for key, value in label_to_int.items():\n",
    "        if value == predicted_class_index:\n",
    "            keys_with_value.append(key)\n",
    "    return keys_with_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 25s 25s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "오리로스구이1.jpg : , Predict : ['해물볶음']\n",
      "생선구이.jpg : , Predict : ['생선구이']\n",
      "스크램블에그2.jpg : , Predict : ['밥']\n",
      "오이2.jpg : , Predict : ['오이']\n",
      "오이3.jpg : , Predict : ['오이']\n",
      "콩나물3.jpg : , Predict : ['야채볶음']\n",
      "치킨너겟1.jpg : , Predict : ['치킨너겟']\n",
      "오리로스구이2.jpg : , Predict : ['빵']\n",
      "치킨너겟3.jpg : , Predict : ['치킨너겟']\n",
      "미역국1.jpg : , Predict : ['미역국']\n",
      "죽3.jpg : , Predict : ['죽']\n",
      "치킨너겟2.jpg : , Predict : ['치킨너겟']\n",
      "바나나.jpg : , Predict : ['바나나']\n",
      "배추김치1.jpg : , Predict : ['배추김치']\n",
      "오리로스구이3.jpg : , Predict : ['야채볶음']\n",
      "돼지갈비찜1.jpg : , Predict : ['갈비찜']\n",
      "배추김치3.jpg : , Predict : ['배추김치']\n",
      "생선구이2.jpg : , Predict : ['빵']\n",
      "삼겹살구이2.jpg : , Predict : ['감자볶음']\n",
      "포도1.jpg : , Predict : ['포도']\n",
      "삼겹살구이.jpg : , Predict : ['무김치']\n",
      "콩나물1.jpg : , Predict : ['야채볶음']\n",
      "바나나2.jpg : , Predict : ['바나나']\n",
      "달걀후라이2.jpg : , Predict : ['달걀후라이']\n",
      "포도3.jpg : , Predict : ['포도']\n",
      "방토.jpg : , Predict : ['방울토마토']\n",
      "스크램블에그1.jpg : , Predict : ['밥']\n",
      "미역국2.jpg : , Predict : ['미역국']\n",
      "미역국3.jpg : , Predict : ['미역국']\n",
      "달걀후라이3.jpg : , Predict : ['달걀후라이']\n",
      "보리빵.jpg : , Predict : ['빵']\n",
      "미역국.jpg : , Predict : ['미역국']\n",
      "배추김치2.jpg : , Predict : ['오믈렛']\n",
      "포도2.jpg : , Predict : ['포도']\n",
      "콩나물2.jpg : , Predict : ['배추김치']\n",
      "돼지갈비찜2.jpg : , Predict : ['갈비찜']\n",
      "죽2.jpg : , Predict : ['죽']\n",
      "오이1.jpg : , Predict : ['오이']\n",
      "달걀후라이1.jpg : , Predict : ['달걀후라이']\n",
      "죽1.jpg : , Predict : ['죽']\n",
      "방토2.jpg : , Predict : ['방울토마토']\n",
      "스크램블에그3.jpg : , Predict : ['밥']\n"
     ]
    }
   ],
   "source": [
    "src = []\n",
    "name = []\n",
    "test = []\n",
    "image_dir = './food_test/'\n",
    "for file in os.listdir(image_dir):\n",
    "    if (file.find('.jpg') != -1):\n",
    "        src.append(image_dir + file)\n",
    "        name.append(file)\n",
    "        test.append(predict_food_category(image_dir + file))\n",
    "\n",
    "\n",
    "for i in range(len(test)):\n",
    "    print(name[i] + \" : , Predict : \"+ str(test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'갈비찜': 0, '감자볶음': 1, '고구마': 2, '고추 장아찌': 3, '국수': 4, '단호박찜': 5, '달걀찜': 6, '달걀후라이': 7, '닭갈비': 8, '된장국': 9, '된장찌개': 10, '떡국': 11, '만두국': 12, '무김치': 13, '미역국': 14, '미역나물': 15, '바나나': 16, '밥': 17, '방울토마토': 18, '배추김치': 19, '부침개': 20, '브로콜리': 21, '비엔나 소시지': 22, '빵': 23, '사과': 24, '삶은 달걀': 25, '삼겹살구이': 26, '상추': 27, '샌드위치': 28, '생선구이': 29, '순두부찌개': 30, '스크램블 에그': 31, '야채볶음': 32, '양배추쌈': 33, '양상추샐러드': 34, '어묵국': 35, '오리구이': 36, '오믈렛': 37, '오이': 38, '우유': 39, '잡채': 40, '조미김': 41, '죽': 42, '찜닭': 43, '참외': 44, '치킨너겟': 45, '콩나물 무침': 46, '토마토': 47, '파프리카': 48, '포도': 49, '해물볶음': 50, '훈제연어': 51}\n"
     ]
    }
   ],
   "source": [
    "print(label_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
